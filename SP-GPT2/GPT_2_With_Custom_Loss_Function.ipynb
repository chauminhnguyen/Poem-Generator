{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT-2 With Custom Loss Function.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9Pv1i7j2hLZ"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install einops\n",
        "# !pip install vncorenlp\n",
        "# !pip3 install fairseq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpdoUppdDg7e",
        "outputId": "d94c2808-89cb-40b2-f355-9a3b597790b2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_zaS4DBDvgu"
      },
      "source": [
        "%%capture\n",
        "%cd '/content/drive/MyDrive'\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nobBd8zqCenM"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from transformers import RobertaTokenizerFast\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer\n",
        "from filelock import FileLock\n",
        "from transformers.utils import logging\n",
        "from typing import Dict, List, Optional\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftiqU9PkyGrn"
      },
      "source": [
        "class ScaleDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaleDotProductAttention, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
        "        batch_size, head, length, d_tensor = k.size()\n",
        "\n",
        "        score = torch.einsum(\"bhid,bhjd->bhij\",q,k)\n",
        "        score = score/math.sqrt(d_tensor)\n",
        "\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -e)\n",
        "\n",
        "        score = self.softmax(score)\n",
        "\n",
        "        v = score @ v\n",
        "\n",
        "        return v, score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5rBQArHyJ10"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.attention = ScaleDotProductAttention()\n",
        "        self.w_q = nn.Linear(d_model, d_model*n_head)\n",
        "        self.w_k = nn.Linear(d_model, d_model*n_head)\n",
        "        self.w_v = nn.Linear(d_model, d_model*n_head)\n",
        "        self.w_concat = nn.Linear(d_model*n_head, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        q, k, v = self.w_q(x), self.w_k(x), self.w_v(x)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.n_head), (q, k, v))\n",
        "\n",
        "        out, attention = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        # 4. concat and pass to linear layer\n",
        "        # out = self.concat(out)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.w_concat(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFw91tT0eAhR"
      },
      "source": [
        "class SelfAttentionLstm(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers,n_head):\n",
        "        super(SelfAttentionLstm, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.multi_attention = MultiHeadAttention(d_model=input_size,n_head=4)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.multi_attention(x)\n",
        "         \n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(\"cuda\")\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to('cuda')\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))  #(batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # seq_length = out.shape[1]\n",
        "        #get embedding of last token represent whole context sentence\n",
        "        out = out[: ,-1, : ]\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8cfdlX2E8Lc"
      },
      "source": [
        "train_path = 'Dataset/train_dataset_24_03.txt'\n",
        "test_path = 'Dataset/valid_dataset_24_03.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tjjj_r-KWTv"
      },
      "source": [
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"Custom Loss/Tokenizer_26_03\", max_len=512)\n",
        "tokenizer.add_tokens('\\n')\n",
        "vocab_size= tokenizer.vocab_size\n",
        "vocab_size = vocab_size + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCUlkmXlhSWv"
      },
      "source": [
        "def add_padding(list_token: list, block_size:int):\n",
        "    tmp_list = [0]* block_size\n",
        "    tmp_list[0:len(list_token)] = list_token\n",
        "    tmp_list[len(list_token):block_size] = [1]*(block_size-len(list_token))\n",
        "    return tmp_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftqTXKGQE9As"
      },
      "source": [
        "logger = logging.get_logger(__name__)\n",
        "class CusTextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This will be superseded by a framework-agnostic approach\n",
        "    soon.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        file_path: str,\n",
        "        block_size: int,\n",
        "        overwrite_cache=False,\n",
        "        cache_dir: Optional[str] = None,\n",
        "    ):\n",
        "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
        "\n",
        "        # num_special_tokens_to_add Returns the number of added tokens when encoding a sequence with special tokens\n",
        "\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(\n",
        "            cache_dir if cache_dir is not None else directory,\n",
        "            \"cached_lm_{}_{}_{}\".format(\n",
        "                tokenizer.__class__.__name__,\n",
        "                str(block_size),\n",
        "                filename,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Make sure only the first process in distributed training processes the dataset,\n",
        "        # and the others will use the cache.\n",
        "        lock_path = cached_features_file + \".lock\"\n",
        "        with FileLock(lock_path):\n",
        "\n",
        "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"rb\") as handle:\n",
        "                    self.examples = pickle.load(handle)\n",
        "                logger.info(\n",
        "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
        "\n",
        "                self.examples = []\n",
        "                with open(file_path, encoding=\"utf-8\") as f:\n",
        "                    total_poem = f.read()\n",
        "                    \n",
        "                split_total_poem = total_poem.split(\"\\n\\n\")\n",
        "                canto_poem = [split_total_poem[x:x+4] for x in range(0, len(split_total_poem), 4)]\n",
        "                canto_poem = [\"\\n\\n\".join(i) for i in canto_poem]\n",
        "\n",
        "                canto_token = [tokenizer.encode(i) for i in canto_poem]\n",
        "                canto_token = [i for i in canto_token if len(i) >= 129 and len(i) <= 140]\n",
        "\n",
        "                for i in canto_token:\n",
        "                  self.examples.append(add_padding(i,block_size=block_size ))\n",
        "\n",
        "\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"wb\") as handle:\n",
        "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                logger.info(\n",
        "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
        "                )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i) -> torch.Tensor:\n",
        "        return torch.tensor(self.examples[i], dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIVvq4REFJUg"
      },
      "source": [
        "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling, LineByLineWithSOPTextDataset\n",
        "\n",
        "def load_dataset(train_path, test_path, tokenizer):\n",
        "    train_dataset = CusTextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=140)\n",
        "     \n",
        "    test_dataset = CusTextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=140)   \n",
        "    \n",
        "    return train_dataset,test_dataset\n",
        "\n",
        "train_dataset,test_dataset = load_dataset(train_path,test_path, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpKSppj3XTxu",
        "outputId": "3537e488-5dc2-4631-d2cb-c087313022d9"
      },
      "source": [
        "len(train_dataset[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKVTa0JOS3LJ",
        "outputId": "a7e64c7a-ae2f-461a-d949-2e097adf0e65"
      },
      "source": [
        "print(train_dataset[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([    0,  1536,  5469,   417,  3707,   731,   705, 11982,   657,   546,\n",
            "         3218,  2175,   508,   992,   955,   469, 11982,  7143,   693,   846,\n",
            "          749,   705,   914, 11982,  1309,  2109,  7031,   785,   395,  1011,\n",
            "         1659,   483, 11982, 11982,   829,   584,   609,  2885,   719,   866,\n",
            "        11982,   354,   834,   504,  1982,  1890,  2518,  2345,   427,   705,\n",
            "        11982,   638,   618,   502,  3017,   503,   413, 11982,  1209,   392,\n",
            "         4391,  1352,  3354,   760,  1946,  1735, 11982, 11982,  8024,  3224,\n",
            "         3460,  1986,  3907,  3541, 11982,  4305,   755,  4156,  2625,   888,\n",
            "          601,   516,   495, 11982,  2256,  2496,  3247,  1911,   878,  1117,\n",
            "        11982,  1469,   394,   501,   927,  1768,  2559,   493,   417, 11982,\n",
            "        11982,  3052,   854,  1578,  2232,   985,   849, 11982,  6127,   437,\n",
            "          705,  1270,   586,   950,  1097,   392, 11982,  2190,   693,  1019,\n",
            "         2586,   395,   372, 11982,  3285,  1140,   600,  3695,  2862,   422,\n",
            "         1894,   493,     2,     1,     1,     1,     1,     1,     1,     1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9u8pgwe0TCy"
      },
      "source": [
        "#Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxVk83M7HEIj"
      },
      "source": [
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(dataset= test_dataset, batch_size= 16, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUuttJGKJu87"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments, GPT2Config, GPT2LMHeadModel,GPT2Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfbM-dVKKaJM"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjGoC3HkJxAj"
      },
      "source": [
        "configuration = GPT2Config(vocab_size=vocab_size,n_layer = 6)\n",
        "poem = GPT2LMHeadModel(configuration).to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47sfShXoPHni"
      },
      "source": [
        "x = torch.randint(0, vocab_size, (32, 140)).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG81Xse4KC7y",
        "outputId": "1afad1c3-8eee-4b4c-c87b-5215fcf9ecdf"
      },
      "source": [
        "start = time.time()\n",
        "outputs = poem.transformer(x)\n",
        "end = time.time()\n",
        "print(end-start)\n",
        "print(outputs[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.12488460540771484\n",
            "torch.Size([32, 140, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kad8Qmu53ICC",
        "outputId": "40036bfe-6a07-4ef5-fed5-747aecca0678"
      },
      "source": [
        "count = 0\n",
        "for i in train_loader:\n",
        "  count = count +1 \n",
        "\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jI2TySj0LW7"
      },
      "source": [
        "#Train GPT-2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgvZE9dlH7cD"
      },
      "source": [
        "lr_rate = 0.00001\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(poem.parameters(), lr_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8Dta5PskWfH"
      },
      "source": [
        "def save_checkpoint(state, filename= \"Custom Loss/gpt_2_custom_loss_v2.pth.tar\"):\n",
        "    print(\"Saving checkpoint\")\n",
        "    torch.save(state,filename)\n",
        "\n",
        "def load_checkpoint(state):\n",
        "    print(\"Load checkpoint\")\n",
        "    poem.load_state_dict(state['state_dict'])\n",
        "    optimizer.load_state_dict(state['optimizer'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9dq8_yR4nis",
        "outputId": "d26a10b4-4947-4466-867b-916a1363fef7"
      },
      "source": [
        "load_checkpoint(torch.load(\"Custom Loss/gpt_2_custom_loss_v3.pth.tar\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load checkpoint\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbDeIF8BZhY2"
      },
      "source": [
        "head_gpt = SelfAttentionLstm(input_size=768,hidden_size=1000, num_layers=2,n_head=4).to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxXDSN3Hoz__"
      },
      "source": [
        "def load_checkpoint_lstm(state):\n",
        "    print(\"Load checkpoint\")\n",
        "    head_gpt.load_state_dict(state['state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhzeeH_vpCeq",
        "outputId": "3ae0c00f-7491-4aae-e93c-3281a2e7a596"
      },
      "source": [
        "load_checkpoint_lstm(torch.load(\"Custom Loss/self_attetion_lstm.pth.tar\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load checkpoint\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFlKGP8nNUUC",
        "outputId": "94124aaa-74ab-4a9f-dcfa-bbf912a53d81"
      },
      "source": [
        "test_input = train_dataset[16].to(device)\n",
        "test_input = test_input.unsqueeze(0)\n",
        "lm_logits = poem(test_input).logits\n",
        "token = torch.argmax(lm_logits, dim= 2)\n",
        "poem_output = tokenizer.decode([0] + token[0].tolist(), skip_special_tokens= False)\n",
        "print(poem_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s>em thôi anh thấy hả ông\n",
            "mần tui xí hụt phải lồng rách quần\n",
            "bửa giờ chuẩn bị ông khuân\n",
            "lên tui dẹo miết làm ngần roài đa\n",
            "\n",
            "liếc ngang nốt bả cười khì\n",
            "õng a õng ẹo kéo ghì cái lưng\n",
            "giả đò làm bộ ngắm hun\n",
            "cái mình cá lắc run run cặp đùi\n",
            "\n",
            "còn mà cũng thấy vui vui\n",
            "vợ hiền vợ ngắm lại thời nhõng nheo\n",
            "tánh mình lam thích vợ cam\n",
            "còn vợ mình thích đi chiều mình ôm\n",
            "\n",
            "còn phanh có chả có ai\n",
            "ngực căng e ấp bờ vai tròn tròn\n",
            "còn gì vợ gái có con\n",
            "chẳng làm quan vẫn làm mòn hết ai</s><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzyTlSgDRpoL"
      },
      "source": [
        "def custom_index(list_token:list):\n",
        "    list_token = [list_token[i:i+4] for i in range(0,len(list_token),4)]\n",
        "    for i in range(len(list_token)):\n",
        "      list_token[i] = [list_token[i][0],list_token[i][3]]\n",
        "    return list_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yehdU-AARLG0"
      },
      "source": [
        "def get_idx_two_line(lm_logits):\n",
        "    token = torch.argmax(lm_logits, dim= 2)\n",
        "    token = token[0].tolist()\n",
        "    index_token = [0]\n",
        "    for i in range(len(token)):\n",
        "        if token[i:i+2] == [11982,11982]:\n",
        "          index_token.append(i)\n",
        "          index_token.append(i+2)\n",
        "    index_token.append(len(token))\n",
        "\n",
        "    # Lấy index đầu và cuối của 1 khổ\n",
        "    index_khotho = [index_token[i:i+2] for i in range(0,len(index_token),2)]\n",
        "    index_khotho = [i for i in index_khotho if len(i) == 2]\n",
        "\n",
        "    a = index_khotho\n",
        "    \n",
        "    #Lấy index của token đầu và token cuối của 2 câu trong 1 khổ\n",
        "    token_final = []\n",
        "    for idx_khotho in index_khotho:\n",
        "        tmp = token[idx_khotho[0]:idx_khotho[1]]\n",
        "        token_tmp = [idx_khotho[0]]\n",
        "        for i in range(len(tmp)):\n",
        "          if tmp[i] == 11982:\n",
        "            token_tmp.append(i + idx_khotho[0])\n",
        "            token_tmp.append(i+1 +idx_khotho[0])\n",
        "        token_tmp.append(idx_khotho[1])\n",
        "        if len(token_tmp) != 8:\n",
        "          continue \n",
        "        else :\n",
        "          token_final.append(custom_index(token_tmp))\n",
        "\n",
        "    # a = [a[i:i+2] for i in range(len(a))]\n",
        "    # a = [i for i in a if len(i) == 2]\n",
        "    \n",
        "    return token_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OgQCmN1cnYL",
        "outputId": "d513beee-3374-4e50-c63d-80b32fceff7f"
      },
      "source": [
        "a = get_idx_two_line(lm_logits) \n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[0, 15], [16, 31]],\n",
              " [[33, 48], [49, 64]],\n",
              " [[66, 81], [82, 97]],\n",
              " [[99, 114], [115, 140]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU1DUX8UUCWI"
      },
      "source": [
        "def loss_kho_tho(lm_logits,embedding):\n",
        "    lm_logits = torch.unsqueeze(lm_logits,0)\n",
        "    pair_list = get_idx_two_line(lm_logits)\n",
        "    embedding = torch.unsqueeze(embedding,0)\n",
        "    \n",
        "    total_lost = 0\n",
        "    loss = nn.MSELoss().to(device)\n",
        "    for pair in pair_list:\n",
        "        one = pair[0]\n",
        "        two = pair[1]\n",
        "\n",
        "        if one == None or two == None:\n",
        "          continue\n",
        "\n",
        "        embedd_one = head_gpt(embedding[:,one[0]: one[1], :])\n",
        "        embedd_two = head_gpt(embedding[:,two[0]: two[1], :])\n",
        "\n",
        "        total_lost += loss(embedd_one,embedd_two)\n",
        "\n",
        "    return total_lost     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6ExJB7cYtuL",
        "outputId": "3e7bfe3a-0235-45b7-c99e-cbdd1491d54f"
      },
      "source": [
        "for i, batch in enumerate(train_loader):\n",
        "  if i == 1:\n",
        "    embedding = poem.transformer(batch.to(device))[0]\n",
        "    lm_logits = poem(batch.to(device)).logits \n",
        "    break\n",
        "\n",
        "start = time.time()\n",
        "print(sum([loss_kho_tho(lm_logits[i],embedding[i]) for i in range(lm_logits.shape[0])]))\n",
        "end = time.time()\n",
        "print(end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0036, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "0.40593647956848145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2XLvo--hFN1"
      },
      "source": [
        "for param in head_gpt.parameters():\n",
        "    param.require_grad = True\n",
        "\n",
        "for param in poem.parameters():\n",
        "    param.require_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUekiaKImJyw",
        "outputId": "990a5de8-7796-4db2-fffd-ef886d824f03"
      },
      "source": [
        "checkpoint = {'state_dict': head_gpt.state_dict()}\n",
        "save_checkpoint(checkpoint, filename= \"Custom Loss/self_attetion_lstm.pth.tar\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving checkpoint\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIgU5LOn4w6r"
      },
      "source": [
        "class TextGenerator():\n",
        "\n",
        "    def __init__(self, max_tokens, start_tokens, maxlen, model, tokenizer,device, topk):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.start_tokens = start_tokens\n",
        "        self.maxlen = maxlen\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.k = topk \n",
        "\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = torch.topk(logits, k=self.k, sorted=True)\n",
        "        return np.random.choice(indices.cpu().numpy())\n",
        "\n",
        "\n",
        "    def gen_poem(self):\n",
        "        start_tokens = [_ for _ in self.start_tokens]\n",
        "        num_tokens_generated = 0\n",
        "        tokens_generated = []\n",
        "        while num_tokens_generated <= self.max_tokens:\n",
        "            pad_len = self.maxlen - len(start_tokens)\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            if pad_len < 0:\n",
        "                x = start_tokens[:self.maxlen]\n",
        "                sample_index = self.maxlen - 1\n",
        "            elif pad_len > 0:\n",
        "                x = start_tokens + [0] * pad_len\n",
        "            else:\n",
        "                x = start_tokens\n",
        "            x = torch.tensor([x], device= self.device)\n",
        "            y = self.model(x).logits\n",
        "            sample_token = self.sample_from(y[0][sample_index])\n",
        "            tokens_generated.append(sample_token)\n",
        "            start_tokens.append(sample_token)\n",
        "            num_tokens_generated = len(tokens_generated)\n",
        "            # print(sample_token)\n",
        "        output_token = [_ for _ in self.start_tokens + tokens_generated]\n",
        "        poem = self.tokenizer.decode(output_token)\n",
        "        print(f\"generated text:\\n{poem}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzgTiwgYmUAn",
        "outputId": "9350f867-50a2-44b8-f4d0-9c1e009cfc3b"
      },
      "source": [
        "num_token_generated = 30\n",
        "hint = 'mùa thu'\n",
        "start_tokens = tokenizer.encode(hint)[:-1]\n",
        "generator = TextGenerator(max_tokens= num_token_generated, start_tokens= start_tokens, maxlen= 300, model= poem, tokenizer= tokenizer,device= device, topk= 1)\n",
        "generator.gen_poem()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generated text:\n",
            "<s>mùa thu gợi lắm những gì\n",
            "còn đang hò hẹn những khi sững sờ\n",
            "mùa thu vắng tiếng trẻ thơ\n",
            "đôi bờ môi khép sững sờ mắt nhau\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}